This repository contains the complete infrastructure for an autonomous, two-round system that verifies a request, uses a large language model (LLM) to generate a full-stack single-page application, deploys the code to GitHub Pages, and reports the deployment status to an external evaluation API.<br>
The entire process is managed via a single, non-blocking Flask API endpoint, ensuring rapid response times essential for high-score evaluation.

⌛ The API returns success in ~1 minute (while the code is being generated by the LLM), but the full deployment takes 1–3 minutes to be live on GitHub Pages. This architecture guarantees the required fast API response.

## ⚙️Tech Stack
- Backend: Python 3.13, Flask 3.0
- LLM: Google generative ai
- Version Control: PyGithub 2.8
- Deployment: Render
- Environment: python-dotenv

## ✔️ Core Features
- RESTful endpoint accepts JSON requests with app specifications.
- Creates repositories, manages commits, and enables GitHub Pages.
- Handles multiple rounds of modifications to existing applications.
- Automatically includes MIT license and comprehensive README for each generated app.
- Notifies evaluation URL with retry logic.
- Returns appropriate HTTP status codes.

## ➿ Workflow
> Request → Verify Secret → Generate Code (Google generative ai) → Create/Update GitHub Repo → Enable Pages → Notify Evaluation URL → Response

## The goal of the mini project

This project demonstrates the practical application of LLM-powered automation in software development workflows. The key learning objectives are:

- **API Development**: Building robust REST APIs that handle complex workflows.
- **LLM Integration**: Leveraging large language models for code generation and automation.
- **CI/CD Automation**: Implementing automated deployment pipelines using GitHub APIs.
- **Error Handling**: Managing asynchronous operations, retries, and failure recovery.
- **End-to-End Workflow**: Orchestrating multiple services (LLM, GitHub, deployment platforms) into a cohesive system.

## Security Measures

- Secrets managed via environment variables.
- GitHub token has minimal required permissions.
- `.env` file excluded from github repo.
- Secret verification on all requests.

## Architecture Overview

The system is designed to handle two distinct phases: Build (Round 1) and Revision (Round 2). The core components are:

### 1. The Core API (app.py)

- Role: Serves as the single entry point. It verifies the student secret, parses the task, orchestrates the LLM generation, and handles the deployment process.<br>
- Key Feature (Asynchronous Pinging): The system uses Python's threading library to execute the POST request to the evaluation_url in the background. This is crucial because:
- The client receives an immediate HTTP 200 OK success message (within ~1 second).
- The expensive, time-consuming evaluation ping (which includes network latency and exponential backoff retries) runs off the main thread.

### 2. The Code Generator (generator.py)

- Role: Interfaces with the Google Gemini API (gemini-2.5-flash).
- Prompt Engineering: Uses a detailed, multi-step system prompt to instruct the LLM to generate a single, self-contained index.html file (including embedded CSS/JS) and adhere to all dynamic requirements and checks.
- Multimodality: Correctly handles data:uri attachments (like images or CSVs) by decoding the Base64 data and sending it as a types.Part object to the Gemini model, allowing the LLM to process visual or data-specific prompts (e.g., solving a captcha).

### 3. The Deployment Manager (githubcode.py)

- Role: Manages repository creation, file commits, and GitHub Pages activation using the PyGithub library.
- Round 1 (Build): Creates a new public repository, commits LICENSE, README.md, and index.html. It then explicitly enables GitHub Pages on the main branch.
- Round 2 (Revision): Locates the existing repository and performs an update commit on README.md and index.html, which triggers an automatic redeployment of GitHub Pages.


